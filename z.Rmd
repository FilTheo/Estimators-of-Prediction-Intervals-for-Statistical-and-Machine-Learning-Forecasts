---
title: "R Notebook"
output: html_notebook
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F, error = F)
``` 

## Introduction

In this work, methods for Statistical Models will be defined and will be tested on the single AirPassengers Time Series.

Later on the next notebooks, methods will be applied on the whole sets of time-series

```{r error=FALSE}
library(forecast)
library(EnvStats) #for geometric mean
library(Metrics) # for MASE
library(ggplot2)
library(smooth)
```

#### Dataset Used : 

The AirPassengers dataset provided by R will initialy be used. A pretty simple dataset with some hidden irregularities, which are more than enough for the selected methods to be initialy compared. 

Moreover,a total of 144 monthly obsevations on  a 12-years period is good-enough sample size.

```{r}
df <- AirPassengers
par(bg = 'aliceblue')
plot(df , main = 'Selected Dataset' , ylab = 'Value' )
print(df)


```


#### Evaluation Function:



The evaluation function I am going to use is Mean Interval Scores.(Kourentzes et al 2019 , Gneiting et al 2004)

Formula:

$$(u-l) + \frac{2}{a}(l-x)*ID(x,l) + \frac{2}{a}(x-u)ID(u,x)$$ 

Where u and l stand for the upper and lower forecasted intervals and x for the true value

ID(a,b) Is a function that returns 1 if a > b and 0 otherwise

**Cross-Validaton**



Dataset will be splitted into two parts, as the initial setup, with  test-set including the last years, approximately the 0.20%(0.17%) of the datatset's size.

Initialy, the model will be fitted into the training dataset which includes values from 1 to 119 and the intervals for the next 12 months (value 120 to 132) will be forecasted and evaluated against the first 12 values of the test-set( value 1 to 12)

On the next step, model will be **re-fitted** on an updated training set which will also include the first observation from the previous test set. A new 12-steps-ahead forecast will take place and will be evaluated. The new forecast is equivelant to values 2 to 13 of the test set.


What is more, Geometric Mean Relative Absolute Error will be used(Athanasopoulos et al , 2020) to  compare the performance ot two models against the optimal model for the selected dataset , on every method.

GMRAE is a method which takes the absolute value of the  Geometric Mean of the Interval score of each model, divided by the equivelant score of the benchmark model, on each method defined . Let IS be the interval score and N the total number of origins : 

$$GMRAE = \sqrt[N]{\prod_{}^{N}\left |\frac{IS_{A}}{IS_{MAM}}\right |}$$

where A stands for the Interval score of either model 1 or model 2. Relative errors lower than 1 indicate that model A outperformed the benchmark model, while scores bigger than 1 show that the optimal one performed better.

**Note**

The method takes the Geometric mean of the mean IS score of the cross-validated results, for each horizon h. As a results there would some extra bias on the final results. 

Another more accurate approach would be to take the Geometric mean of the IS scores on every horizon and get a GMRAE values for each horizon rather than an averaged result. However, as it would be computationaly expensive and the difference on the IS scores between MAM and the other two models has been proved, the selected approach was picked.


#### Models to be compared

The different methods would be implemented and compared on a model of each one of the three classes defined by Hyndman ,2008

**Classes : **

 **Class 1** : Linear Homoscedastic. -> Additive Errors with additive or none Seasonality and Trend
 
              (1) Linear models with Additive Errors. 
              
              (2) Additive errors typicaly that the variance is constant over time .
              
              (3) Error term is added to the state-space and forecast equation through addition
               
 The model that will be used is  ETS(A,A,A) (AAA) -> Additive Errors,Seasonality,Trend
               
 
 **Class 2** : Linear Heteroscedastic -> Multiplicative Errors with additive or non Seasonality and Trend

              (1) Linear Models With multiplicative Errors
              
              (2) Multiplicative errors are treated as relatives errors and they  imply an increase in the std proportional to the mean level!
              
              (3) Error term is added through multiplication
              
 The model that will be used is ETS(M,A,A) (MAA)

**Despite the similarities between models of Class 1 and Class 2 and the fact that their forecast mean is identical(under the assumption that they have the same parameters), different prediction intervals will be computed **(Hyndman 2008 )


**Class 3** : Non Linear Models with Multiplicative Errors

             Similar to Class 2 with multiplicative seasonality
          
 Model to be used ETS(M,A,M) -> 
 
 **Note**
 MAM is the optimal, benchmark model for the selected Dataset as it has the best scores in terms of AICc from any other available model.
          
For the rest of the models which include models with multiplicative trend or a mixture of Additive errors and multiplicative seasonality(or vice versa) there are no algebric formulas and a simulation method which will be described bellow is recommended instead of the theoretical one.(Hyndman  2018)




## Methods.

#### (1) Method Theoritical-algebric method!


In general for most models prediction intervals are given by : 
$$[\hat{y}_{t+h|t} - c\sigma(h) ,  \hat{y}_{t+h|t} + c\sigma(h) ]$$

where y is point forecast , c is the coverage probablity and sigma(h) is the forecast's variance.(Hyndman 2008)

Getting the actuals values for y_hat and sigma(h) is feasible as long as some assumptions defined on this repository's readme are made


**Note 1**

For models of class 3,which are not linear, we have to approximate the variance and the mean as the actuals values are complicated to be computed for h >= m, with m being the number of periods for each season.(Hyndman 2008)

For h <= m the approximations are exact. 

**Note 2**

This method cannot be used for models outside of these three classes, as either a formula does not exist or there are many numerical difficulties on the calculations. (Hyndman 2008,2018)

To calculate the mean and the variance for each model and thus, the PIs we use the formulas given by Hyndman 2008.

```{r}
#the ID function for IS bellow
Id <- function(a , b){
  if (a > b) 1
  else 0
}
#Interval Score for a single Interval
IS <- function (true, upper , lower , a){ #Interval Score
  (upper - lower ) + 2/a *(lower - true)*Id(lower,true) + 2/a*(true - upper)*Id(true,upper)
}
#Mean Interval Score
ISs <- function(true,upper,lower,a){ 
  ISs <- rep(0,length(true))
  for (i in 1:length(ISs)){
    
    ISs[i] <- IS(true[i],upper[i],lower[i],a)
  }
  ISs #returns Interval Scores
}
```



```{r}
#Algebric cross validation :
cross_val_algebric <- function(model , h ){
  #Initializing
  train <- window(AirPassengers , end = 1958.99)
  test <- window(AirPassengers , start = 1959)
  n <- length(test) - h + 1
  
  lower <- rep(NA , 12)
  upper <- rep(NA , 12)
  #Every row is the IS of a 12-steps-ahead forecast
  IS_mat <- matrix(NA , nrow = n , ncol = h)
  fit <- ets(train , model = model)
  for(i in 1:n){
  #Adjusting the training set to include a new observation each time!
    #New training set:
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    #New test set:
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    #Refitting:
    refit <- ets(x , model = model)
    #New forecast:
    new_forecast <- forecast(refit , h = h)
    #New intervals:
    lower <- new_forecast$lower[,2]
    upper <- new_forecast$upper[,2]
    IS_mat[i , ] <- ISs(test , upper , lower , 0.05)
    #Every row is a forecast for h = 12 
    #In total I have 13 forecasts to evaluate
  }
  #The mean of every collumn is the mean IS for every every horizon h
  Mean_IS <- colMeans(IS_mat)
return(Mean_IS)
}
#Defining train and test set to show results on a single origin
train_set <-ts(df,frequency=12,start=c(1949,1),end=c(1959,12))
test_set <-as.double(ts(tail(df,12),frequency=12,start=c(1960,1),end=c(1960,12)))
#Fitting the models

#AAA <- ets(train_set, model = 'AAA' )
#MAA <- ets(train_set , model = 'MAA')
MAM <- ets(train_set , model = 'MAM')
#Getting point forecasts for the last origin

#algebricAAA <- forecast(AAA , 12)
#algebricMAA <- forecast(MAA , 12)
algebricMAM <- forecast(MAM , 12)
```


**Results of the CV Algebric Method:**

```{r}
#Getting cross validation results
h <- seq(1,12)
alg_res <- data.frame(h)
#Cross validated results
alg_res['MAM_cv'] <-cross_val_algebric('MAM' , 12)
print.data.frame(alg_res)

#alg_res['MAA_cv'] <-cross_val_algebric('MAA' , 12)
#alg_res['AAA_cv'] <-cross_val_algebric('AAA' , 12)
```



And the forecasted intervals for the whole last year :

```{r}
#Getting the intervals

#upperAAA <- algebricAAA$upper[,2]
#lowerAAA <- algebricAAA$lower[,2]

#upperMAA <- algebricMAA$upper[,2]
#lowerMAA <- algebricMAA$lower[,2]

upperMAM <- algebricMAM$upper[,2]
lowerMAM <- algebricMAM$lower[,2]

```


```{r}
up <- upperMAM
low <- lowerMAM
par(bg = 'aliceblue')
plot(c(1:12),test_set , type='l' , lwd = 3 , col='red' , xlab='Horizon' , ylab ='Value' ,ylim=c(300,700) ,
     main = "Theoreticaly Forecasted Interval of MAM model", sub='Figure 1.4',cex.sub=0.7 , font.main=3 , col.sub='red3')
lines(c(1:12),up,lwd=2)
lines(c(1:12),low,lwd=2)
lines(c(1:12) , algebricMAM$mean , lwd = 3 ,col = 'blue')
polygon(c(c(1:12), rev(c(1:12))),c(up, rev(low)), 
        density = c(20, 30),
     col = 'yellow3', border = NA)

legend(0.7,650,legend = c("True","Predicted") , col = c("red" , "blue "),lty=1 )
```

#### (2) Method -> Simulation



A very simple approach to get the desired intervals is to simulate future paths for each model and approximate the prediction distribution (Hyndman 2008). From the prediction distribution we can get the desired percentiles 



**Getting the Intervals through Simulation**
  
  (1) Simmulate M sample paths from the model by forecasting M times.
  
  (2) Assume Gaussian(by default,can be modified) distribution for the in-sample historical errors
  
  (3) For each simulation Mi, generates a value et from the sample of errors using rnorm1
  
  (4) For Mi get the prediction with the "sampled" errors on the step above:
    $$\mathbf{\hat{y}} =  [ \hat{y}_{n+h}(1) , \hat{y}_{n+h}(2) , ... , \hat{y}_{n+h}(M)  ] $$
  
  (5) Estimate the distribution of the above set and get the quantiles.
  
**Important Note**

Different methods to estimate this distribution do exist and will be explored.

  (6) Calculate the intervals



A simple example of how this method works: 

**Note :** The simulate function uses normal distributed errors by default and for now this is what will be used in this paper


```{r}
simmulations <- (matrix(0, 12 , 30))
for (i in 1:30){
  simmulations[,i] <- as.double(simulate(AAA , 12 ))
}


train_set1 <- as.vector(train_set)
#par(bg = 'aliceblue')
plot(train_set1, xlim = c(1,144) , ylim = c(100, 600), type = 'l',ylab='',xlab='' , lwd = 2 )
for (i in 1:30){
   lines(133:144, simmulations[,i] ,col = "indianred1" ,lwd = 0.1 )}
#abline(v = 144 , col="red", lwd = 3, lty = 1 , )
title(main = "30 simulated paths")
legend("topleft", legend = c("Training Set", "Simmulations") ,lty = 1, col = c("black","indianred1"), cex=0.8) 

```

Thirty simulated 12-steps-ahead paths. 

The same procedure will be repeated for a total of 10.000 paths.

After the procedure has been repeated for every model, different methods will be used, for the desired interval to be calculated.

The distribution of the simmulations for the last month of the test set 
```{r, fig.height=6, fig.width=8}

#simmulationsAAA <- (matrix(0, 12 ,10000))
#for (i in 1:10000){
#  simmulationsAAA[,i] <- simulate(AAA , 12 )
#}

#simmulationsMAA <- (matrix(0, 12 ,10000))

#for (i in 1:10000){
#  simmulationsMAA[,i] <- simulate(MAA , 12 )
#}

simmulationsMAM <- (matrix(0, 12 , 10000))
for (i in 1:10000){
  simmulationsMAM[,i] <- simulate(MAM , 12 )
}

#Same lims to c ompare both x and y 
par(bg = 'aliceblue')
#hist(simmulationsAAA[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(250,600),main='AAA' )
#hist(simmulationsMAA[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(150,650),main='MAA' )
hist(simmulationsMAM[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(250,600),main='MAM' )
```

In order to get the 95% prediction intervals from the predicted distributions, different approaches could be made so a more accurate estimation could be performed.

The following approaches will be considered: 


**(1) Direct Empirical :** Directly estimate the quantiles of the samplse by using an empirical method which returns the values that lie within the 5%th and the 95%th observations 

**Note** On the first experiment, a direct empirical method gave better results than the direct KDE method. However, after considering the literature, which proved that the KDE performs better than the direct empirical on a good-enough sample size, a direct KDE method is recommended.

**(2) Mean - sigma:** Get the mean forecast , which stands for the mean value of the distribution, with a small deviation, calculate the spread of errors around the mean by getting their std and calculate the interval.small deviation, calculate the spread of errors around the mean and get the quantile -

**(3) Mean - empirical: ** Get the mean forecast,calculte the errors around and use an empirical method to get the quantiles! Then sum up and down on the mean forecast. To get the quantiles we get the values that lies between the 5% and the 95% observations. In other words, for a set of numbers such as [1, 2, ... ,100] the interval is [5,95]

**(4) Mean - KDE :** Get the mean forecast, calculate the density function of the errors using KDE and approximate the perchentiles, then sum up and down from the mean value and get the interval!

The results of the different methods on the distributions above : 

```{r}
#Direct quantile for the given level and data
direct_quant <- function(data , level){
  prob <- c(1-level , level)
  quant <- quantile(data , prob)
  lower <- as.double(quant[1])
  upper <- as.double(quant[2])
  return(list(lower,upper))
}
#direct_quant(simmulationsAAA[12,],0.05)[1]

#Second method -> mean and sigma
mean_sigma <- function(data , level, mu_forecast=mean(data) ){
  
  #mu_prediction <- mean(data)
  z <- qnorm(c((1-0.95)/2,(1+0.95)/2))
  #errors <- data - mu_forecast
  sigma <- sqrt(mse(mu_forecast,data))
  quant <- mu_forecast + sigma * z
  lower <- as.double(quant[1])
  upper <- as.double(quant[2])
  return(list(lower,upper)) }
#mean_sigma(simmulationsAAA[12,],0.95)  

#Third method -> mean empirical
mean_empirical <- function(data , level, mu_forecast=mean(data) ){
  probs <- c(1-level,level)
  errors <- data - mu_forecast
  error_quant <- quantile(errors, prob =probs )
  quant <- mu_forecast + error_quant
  lower <- quant[1]
  upper <- quant[2]
  return(list(lower,upper)) }

#Forth method -> KDE
mean_kde <- function(data , level , mu_forecast = mean(data)){
  #using Silvermans bandwidth and epanechnikov kernel
  errors <- data - mu_forecast
  kde <- density(errors , bw = 'nrd0' , kernel ="epanechnikov")
  kcde <- cumsum(kde$y)/max(cumsum(kde$y))
  q <- (1-level)/2 + c(0,1)*level
  quant <-rep(0,2)
  for (j in 1:2){
      idx <- order(abs(kcde-q[j]))[1:2]
      quant[j] <- approx(kcde[idx],kde$x[idx],xout=q[j])$y
    
  }
  lower <- mu_forecast + quant[1]
  upper <- mu_forecast + quant[2]

  return(list(lower,upper)) }
```

```{r}

#method1AAA <- direct_quant(simmulationsAAA[12,] , 0.95)
#method2AAA <- mean_kde(simmulationsAAA[12,] , 0.95)

#method1MAA <- direct_quant(simmulationsMAA[12,] , 0.95)
#method2MAA <- mean_kde(simmulationsMAA[12,] , 0.95)

method1MAM <- direct_quant(simmulationsMAM[12,] , 0.95)
method2MAM <- mean_kde(simmulationsMAM[12,] , 0.95)
method3MAM <- mean_sigma(simmulationsMAM[12,] , 0.95)
```

**The different Intervals extracted by the different methods (for h = 12) **

```{r, fig.height=4, fig.width=10}
par(mfrow=c(1,3),bg = 'aliceblue')
hist(simmulationsMAM[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(250,600) 
     , main='MAM with Direct-Empirical' ,freq = FALSE)
abline(v=method1MAM[1], col="green", lwd=3, lty=2 )
abline(v=method1MAM[2], col="green", lwd=3, lty=2)

hist(simmulationsMAM[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(200,600) 
     , main='MAM with Mean-KDE' ,sub='Figure 2.1',cex.sub=1.5 ,col.sub='red3',freq = FALSE )
abline(v=method2MAM[1], col="green", lwd=3, lty=2 )
abline(v=method2MAM[2], col="green", lwd=3, lty=2)
dens <- density(simmulationsMAM[12,], bw = 'nrd0' , kernel ="epanechnikov")
lines(dens,col = 'blue' ,lwd=3 )

hist(simmulationsMAM[12,] ,breaks=50, col='red' ,xlab ='Observation' , xlim=c(250,600) 
     , main='MAM with Mean-Sigma',freq = FALSE )
abline(v=method3MAM[1], col="green", lwd=3, lty=2 )
abline(v=method3MAM[2], col="green", lwd=3, lty=2)

```
As one can observe, different intervals were produced by the different methods

As a results, choosing the best method to calculate the interval from the sample of simmulated paths is very important


**The Cross-Validated Results : **

```{r}
#Defining the functions:

full_direct_quant <- function(data , level){
  upper <- seq(0,12)
  lower <- seq(0,12)
  prob <- c(1-level , level)
  for (i in 1:12){
    quant <- quantile(data[i,] , prob)
    lower[i] <- as.double(quant[1])
    upper[i] <- as.double(quant[2])
  }
  return(list(lower[1:12],upper[1:12]))
}
#Second method -> mean and sigma
full_mean_sigma <- function(data , level) {
  upper <- seq(0,12)
  lower <- seq(0,12)
  z <- qnorm(c((1-0.95)/2,(1+0.95)/2))
  for (i in 1:12){
    mu_forecast <- mean(data[i,])
    #errors <- data - mu_forecast
    sigma <- sqrt(mse(mu_forecast,data[i,]))
    quant <- mu_forecast + sigma * z
    lower[i] <- as.double(quant[1])
    upper[i] <- as.double(quant[2])
  }
  return(list(lower[1:12],upper[1:12])) }
#mean_sigma(simmulationsAAA[12,],0.95)  

#Third method -> mean empirical
full_mean_empirical <- function(data , level ){
  
  lower <- seq(0,12)
  upper <- seq(0,12)
  probs <- c(1-level,level)
  for (i in 1:12){
    mu_forecast = mean(data[i,])
    errors <- data[i,] - mu_forecast
    error_quant <- quantile(errors, prob =probs )
    quant <- mu_forecast + error_quant
    lower[i] <- quant[1]
    upper[i] <- quant[2]
  }
  return(list(lower[1:12],upper[1:12])) }

#Forth method -> KDE
full_mean_kde <- function(data , level ){
  #using Silvermans bandwidth and epanechnikov kernel
  upper <- seq(0,12)
  lower <- seq(0,12)
  q <- (1-level)/2 + c(0,1)*level
  for (i in 1:12){
    mu_forecast <- mean(data[i,])
    errors <- data[i,] - mu_forecast
    kde <- density(errors , bw = 'nrd0' , kernel ="epanechnikov")
    kcde <- cumsum(kde$y)/max(cumsum(kde$y))
    q <- (1-level)/2 + c(0,1)*level
    quant <-rep(0,2)
    
    for (j in 1:2){
        idx <- order(abs(kcde-q[j]))[1:2]
        quant[j] <- approx(kcde[idx],kde$x[idx],xout=q[j])$y
    
  }
    lower[i] <- mu_forecast + quant[1]
    upper[i] <- mu_forecast + quant[2]
}
  return(list(lower[1:12],upper[1:12])) }

#Function that return IS scores for 12-steps-ahead for each one of the 4 methods
#Will be used 13 times!
single_row_results <- function(fitted_model ,test_set , h ,to_bootstrap=FALSE ){
  #Initializing
  direct_quant <- rep(NA , 12)
  Mean_Sigma <- rep(NA , 12 )
  Mean_Emp <- rep(NA , 12 )
  Mean_KDE <- rep(NA , 12)
  flag <- TRUE
  while (flag == TRUE){
    flag <- FALSE
    simmulations <- (matrix(0, 12 ,10000))
    for (i in 1:10000){
      #Simmulating for the given fitted model and the given h
      simmulations[,i] <- simulate(fitted_model, h ,bootstrap = to_bootstrap)
    }
    #Getting the results
    method1 <- full_direct_quant(simmulations , 0.95)
    method2 <- full_mean_sigma(simmulations , 0.95)
    method3 <- full_mean_empirical(simmulations , 0.95)
    method4 <- full_mean_kde(simmulations , 0.95)
    for (i in 1:2){
      if (sum(is.na(method4[[i]]))!=0  ||
          sum(is.na(method3[[i]]))!=0 ||
          sum(is.na(method2[[i]]))!=0 ||
          sum(is.na(method1[[i]]))!=0 ){
        flag <- TRUE
   
      } }
  }
  #Setting the return values
    direct_quant <- c(ISs(test_set,method1[[2]],method1[[1]],0.05))
    Mean_Sigma <- ISs(test_set,method2[[2]],method2[[1]],0.05)
    Mean_Emp <- ISs(test_set,method3[[2]],method3[[1]],0.05)
    Mean_KDE <- ISs(test_set,method4[[2]],method4[[1]],0.05)
    
    #return the results
    return(list(direct_quant,Mean_Sigma,Mean_Emp,Mean_KDE))
    }

cross_val_simmulation <- function(model , h , to_bootstrap){
  #A dataframe for all scores
  to_return <- data.frame(h = c(1:12))
  #Initializing
  train <- window(AirPassengers , end = 1958.99)
  test <- window(AirPassengers , start = 1959)
  n <- length(test) - h + 1
  lower <- rep(NA , 12)
  upper <- rep(NA , 12)
  #A matrix for each on of the methods
  DirectEMP_mat <- matrix(NA , nrow = n , ncol = h)
  MeanSigma_mat <- matrix(NA , nrow = n , ncol = h)
  MeanEMP_mat <- matrix(NA , nrow = n , ncol = h)
  MeanKDE_mat <- matrix(NA , nrow = n , ncol = h)
  fit <- ets(train , model = model)
  for(i in 1:n){
    #Initializing cross validation 
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    refit <- ets(x , model = model)
    #Simmulating for the fitted model
    sim_results <- single_row_results(refit , test , h , to_bootstrap)
    DirectEMP_mat[i,] <- sim_results[[1]]
    MeanSigma_mat[i,] <- sim_results[[2]]
    MeanEMP_mat[i,] <- sim_results[[3]]
    MeanKDE_mat[i,] <- sim_results[[4]]
  }
  to_return['Direct EMP'] <- colMeans(DirectEMP_mat)
  to_return['Mean Sigma'] <- colMeans(MeanSigma_mat)
  to_return['Mean EMP'] <- colMeans(MeanEMP_mat)
  to_return['Mean KDE'] <- colMeans(MeanKDE_mat)
  return(to_return) }

```

```{r}
#AAAsim_res <- cross_val_simmulation('AAA' , 12 , FALSE)
#MAAsim_res <- cross_val_simmulation('MAA' , 12 , FALSE)
MAMsim_res <- cross_val_simmulation('MAM' , 12 , FALSE)
```

For the optimal MAM model :

```{r, fig.height=5, fig.width=8}
no_mean_df <- MAMsim_res[1:12,]
no_mean_df['Algebric'] <- alg_res$MAM_cv[1:12]
par(bg = 'aliceblue')
plot(no_mean_df$h , no_mean_df$`Direct EMP`, type='l', col='red' , ylim=c(40,250) ,pch = 0,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=3)
lines(no_mean_df$h , no_mean_df$`Mean Sigma` , col = 'blue ' ,lwd=2 ,type='o' ,pch=1)
lines(no_mean_df$h , no_mean_df$`Mean EMP` , col = 'yellow ',lwd=2 ,type='o' ,pch=2)
lines(no_mean_df$h , no_mean_df$`Mean KDE` , col = 'purple ' ,lwd=2 ,type='o',pch=3)
lines(no_mean_df$h , no_mean_df$`Algebric` , col = 'green',lwd=2 ,type='o',pch=4)
title(main = "MAM simmulation Interval Scores" , sub='Figure 2.5',cex.sub=0.9 , font.main=3 , col.sub='red3')

legend(0.7 , 250 , legend = c("Direct Quant" , "Mean + Sigma" ,"Mean Empirical","Mean KDE"  ,"Algebric forecast")
         ,col=c("red", "blue" , "yellow" ,"purple" ,"green"),lty=1, cex=0.8 , pch=c(0,1,2,4)) 


print(no_mean_df)
```


Best Results so far!

Interestingly Mean-KDE performed simmilar scores with the algebric method and it even outperformed it for h between 3 and 8. 

What is more, the empirical methods performed pretty bad for smaller h but for larger horizons it outperformed the best Mean-KDE method. As we can see on figure 2.1 despite the fact the density function almost accuretately captured the whole distribution, the empirical methods produced narrower intervals. 


#### (3)Method -> Bootstrap simmulation method




Follows a simmilar idea with the previous method as we use simmulations to get forecast values and out of the set with the forecasted values, we estimate the desired interval(Hyndman 2008)

However, a major difference is that there no need for any assumptions regarding the distribution of the errors!

Instead, we assume that the forecast errors are uncorrelated,a similar assumption to the first method. In case this assumption is violated that means there is some infromation left in the errors which was not accounted in order for the forecast to be optimized. In simple words, intervals wont be optimal. (Hyndman 2018)

Fixing the correlation problem between the errors is a tricky process and more information can be found on Hyndman et al 2018.

Since we are making no assumption about the distribution of the errors: 

**Future Errors are picked through re-samping of the in-sample historical errors**.
  
  (1) Future errors are assumed to be follow a simmilar behaviour with past errors
  
  (2) The process of picking sample Mi is uncorrelated to picking sample Mi-1


In general , instead of generating a random number from a Gaussian Distribution(or any other assumed distribution) for the errors we resamples the values et from the historical sample of fitted values  

When a significant number of future paths has been simulated , approaches similar to the ones used on the second method will be applied for the desired interval to be picked.


**The results for the different models : **


```{r}
#AAAboot_res <- cross_val_simmulation('AAA' , 12 , TRUE)
#MAAboot_res <- cross_val_simmulation('MAA' , 12 , TRUE)
MAMboot_res <- cross_val_simmulation('MAM' , 12 , TRUE)

```


```{r, fig.height=6, fig.width=8}
no_mean_df <- MAMboot_res[1:12,]
no_mean_df['Algebric'] <- alg_res$MAM_cv[1:12]
no_mean_df['Sim Mean-KDE'] <- MAMsim_res$`Mean KDE`
par(bg = 'aliceblue')
plot(no_mean_df$h , no_mean_df$`Direct EMP`, type='l', col='red' , ylim=c(40,250) ,pch = 0,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=3)
lines(no_mean_df$h , no_mean_df$`Mean Sigma` , col = 'blue ' ,lwd=2 ,type='o' ,pch=1)
lines(no_mean_df$h , no_mean_df$`Mean EMP` , col = 'yellow ',lwd=2 ,type='o' ,pch=2)
lines(no_mean_df$h , no_mean_df$`Mean KDE` , col = 'purple ' ,lwd=2 ,type='o',pch=3)
lines(no_mean_df$h , no_mean_df$`Algebric` , col = 'green',lwd=2 ,type='o',pch=4)
lines(no_mean_df$h , no_mean_df$`Sim Mean-KDE` , col = 'black',lwd=2 ,type='o',pch=5)
title(main = "MAM bootstrap Interval Scores" , sub='Figure 3.3',cex.sub=0.9 , font.main=3 , col.sub='red3')

legend("bottomright" ,legend = c("Direct Quant" , "Mean + Sigma" ,"Mean Empirical","Mean KDE"  ,"Algebric forecast","Simmulation Mean-KDE")
         ,col=c("red", "blue" , "yellow" ,"purple" ,"green","black"),lty=1, cex=0.8 , pch=c(0,1,2,4,5)) 


print(no_mean_df)
```


Simmulation method,which assumes normal distribution, performs better for h < 9,while for the later horizons Bootstrap-KDE performs evenly well. Assuming normal distribution has clearly some effect on the scores of this model, but not a significant one.

This is encouraging since assuming normal distribution is something that is often not feasible


If the assumptio of normal distribution cannot be overlapped, and their is an algebric formula for the optimal model, theoretical approaches are far less computationaly expensive and produce equal results.

To distinguish between Mean-KDE and Direct Empirical for the calculation of the interval, sample size is the important factor.


###Empirical Methods


Another "family" of methods, which is based on the hypothesis that interval forecasts can be estiamated by evaluating historical errors. (Isengildina et al , 2006 , Trapero , 2016 , Kourentzes et al , 2019)

As with most theoretical methods, the main assumptiom here is that future forecast errors follow aproximately the same distribution with past errors. Results from past studies pointed out that making this assumption is feasible as past forecast errors gave good predictions for future forecast errors(Smith et al 1988). As a result of this assumption, exploring the behavior of past errors through time, leads to an empirical estimation of  the future errors' distribution.

In order to get an accurate estimation of the distribution of future errors,based on historical ones, the sample size of the historical errors should be large enough. The bigger the sample, the more accurate the estimation. (Isengildina et al,2006)


As with the methods above, finding the upper and lower interval from the estimated distribution is the most important step. Two non-parametric approaches will be used as they were defined above.

**(1)Empirical Percentile**  

Similar to the simple quantile method defined in simmulation and boostrap methods, the values that lie between the 0.05 and 0.95 observation would be taken

**(2)Kernel density estimator**

Getting the propability density function of the forecast errors. Its formula for series X on point x:
$$f(x) = \frac{1}{Nh}\sum_{j=1}^{N}K(\frac{x-X_{j}}{h}) $$


were N is the sample size, K is the kernel smoothing function and h is the bandwidth. This paper uses Silverman's bandwidth and  Epanechnikov kernel, as proposed by Trapero, 2016 and Kourentzes et al, 2019

**(3) Mean-Sigma** 

Another approach that could be used to obtained the desired intervals. A straighforward method which calculates the standard deviation of the errors on each horizon and sums them with the mean forecast! On this case study, we wont consider this method and it can included as future work!

```{r}
#Creating the functions that will produce the intervals!
direct_quant <- function(model , a ){
  a <- (1-0.95)/2 + c(0,1)*0.95
  #Getting the errors:
  errors <- model$errors
  er <- errors[-(1:12),]
  #initializing
  low <- rep(0,12)
  up <- rep(0,12)
  lower <- rep(0,12)
  upper <- rep(0,12)
  #Calculating for each h
  for (h in 1:12){
    low[h] <- as.double( quantile( er[,h],a))[1] 
    up[h] <- as.double( quantile( er[,h],a))[2]
  }
  mu_forecast <- forecast(model , 12)
  
  lower <- mu_forecast$mean + low
  upper <- mu_forecast$mean + up
  return(list(lower[1:12],upper[1:12]))
}


kde_quant <- function(model , a ){
  a <- (1-a)/2 + c(0,1)*a 
  errors <- model$errors
  er <- errors[-(1:12),]
  low <- rep(0,12)
  up <- rep(0,12)
  for (h in 1:12){
    density <- density.default(er[,h],bw='nrd0',kernel = "epanechnikov")
    cdensity <- cumsum(density$y)
    kcde <- cdensity/max(cdensity)
    
    x <- rep(0,2)
    for (i in 1:2){
      idx <- order(abs(kcde-a[i]))[1:2] 
      x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y 
    }
      if (is.na(x[i])){
        idx <- order(abs(kcde-a[i]))[2:3] 
        x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y         
      }
    low[h] <- x[1]
    up[h] <- x[2]
  }
  mu_forecast <- forecast(model , 12)
  
  lower <- mu_forecast$mean + low
  upper <- mu_forecast$mean + up
  return(list(lower[1:12],upper[1:12]))  
  }
#Reference: trnnick , (2017),TStools -> intervals-empirir , GitHub repository https://github.com/trnnick/TStools/blob/master/R/intervals-empir.


#Not sure if this is correct
mu_sigma <- function(model , level){
  a <- qnorm(c((1-level)/2,(1+level)/2))
  errors <- model$errors
  er <- errors[-(1:12),]
  low <- rep(0,12)
  up <- rep(0,12)
  lower <- rep(0,12)
  upper<-rep(0,12)
  sigma <- rep(0,12)
  for (h in 1:12){
    sigma[h] <- sd(er[,h])
    low <- sigma[h]*a[1]
    up <- sigma[h]*a[2]
    
  }
  mu_forecast <- forecast(model , 12)
  lower <- mu_forecast$mean + low
  upper <- mu_forecast$mean + up
  return(list(lower[1:12],upper[1:12]))

  
}

#For multiplicative models.
#Not sure if this is correct
mu_direct_quant <- function(model , a ){
  a <- (1-0.95)/2 + c(0,1)*0.95
  #Getting the errors:
  errors <- model$errors
  er <- errors[-(1:12),]
  #initializing
  low <- rep(0,12)
  up <- rep(0,12)
  lower <- rep(0,12)
  upper <- rep(0,12)
  #Calculating for each h
  for (h in 1:12){
    low[h] <- as.double( quantile( er[,h],a))[1] 
    up[h] <- as.double( quantile( er[,h],a))[2]
  }
  mu_forecast <- forecast(model , 12)
  
  lower <- mu_forecast$mean + low * mu_forecast$mean
  upper <- mu_forecast$mean + up * mu_forecast$mean
  return(list(lower[1:12],upper[1:12]))
}

mu_kde_quant <- function(model , a ){
  a <- (1-a)/2 + c(0,1)*a 
  errors <- model$errors
  er <- errors[-(1:12),] 
  low <- rep(0,12)
  up <- rep(0,12)
  for (h in 1:12){
    density <- density.default(er[,h],bw='nrd0',kernel = "epanechnikov")
    cdensity <- cumsum(density$y)
    kcde <- cdensity/max(cdensity)
    
    x <- rep(0,2)
    for (i in 1:2){
      idx <- order(abs(kcde-a[i]))[1:2] 
      x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y 
      if (is.na(x[i])){
        idx <- order(abs(kcde-a[i]))[2:3] 
        x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y         
      }
    }
    low[h] <- x[1]
    up[h] <- x[2]
  }
  mu_forecast <- forecast(model , 12)
  
  lower <- mu_forecast$mean + low * mu_forecast$mean
  upper <- mu_forecast$mean + up * mu_forecast$mean
  return(list(lower[1:12],upper[1:12]))  
  }







cross_val_empirical <- function(model , h ,multi_error=TRUE){
    to_return <- data.frame(h = c(1:12))
    #Initializing
    train <- window(AirPassengers , end = 1958.99)
    test <- window(AirPassengers , start = 1959)
    n <- length(test) - h + 1
    lower <- rep(NA , 12)
    upper <- rep(NA , 12)
    #A matrix for each on of the methods
    MSE_means <- rep(NA,n)
    MSE_mat <- matrix(NA , nrow = n , ncol = h)
    EMP_mat <- matrix(NA , nrow = n , ncol = h)
    KDE_mat <- matrix(NA , nrow = n , ncol = h)
    fit <- ets(train , model = model)
    #Converting to es
    model_es <- es(train , model = fit , h = 12)
  for(i in 1:13){
    #Initializing cross validation 
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    refit <- ets(x , model = model)
    refit_es <- es(x , model = refit , h = 12)
    #Getting results for each origin
    if (multi_error==TRUE){
      upper_emp <- mu_direct_quant(refit_es , 0.95)[[2]]
      lower_emp <- mu_direct_quant(refit_es , 0.95)[[1]]
      EMP_mat[i,] <- ISs(test, upper_emp , lower_emp , 0.05)
      
      upper_kde <- mu_kde_quant(refit_es , 0.95)[[2]]
      lower_kde <- mu_kde_quant(refit_es , 0.95)[[1]]
      KDE_mat[i,] <- ISs(test, upper_kde , lower_kde , 0.05)
    }
    else {
      upper_emp <- direct_quant(refit_es , 0.95)[[2]]
      lower_emp <- direct_quant(refit_es , 0.95)[[1]]
      EMP_mat[i,] <- ISs(test, upper_emp , lower_emp , 0.05)
      
      upper_kde <- kde_quant(refit_es , 0.95)[[2]]
      lower_kde <- kde_quant(refit_es , 0.95)[[1]]
      KDE_mat[i,] <- ISs(test, upper_kde , lower_kde , 0.05)      
    }

  }
  
  to_return['Direct Empirical'] <- colMeans(EMP_mat)
  to_return['KDE Empirical'] <- colMeans(KDE_mat)  
  return(to_return)
  #return(KDE_mat)
}

cross_val_mse <- function(model , h ){
    
    #Initializing
    train <- window(AirPassengers , end = 1958.99)
    test <- window(AirPassengers , start = 1959)
    n <- length(test) - h + 1

    #A matrix for each on of the methods
    MSE_means <- rep(NA,n)
    MSE_mat <- matrix(NA , nrow = n , ncol = h)
    for(i in 1:13){
      #Initializing cross validation 
      x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
      test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
      refit <- ets(x , model = model)
      refit_es <- es(x , model = refit , h = 12)
      point <- forecast(refit_es , 12)
      MSE_mat[i,] <- (test-point$forecast)^2
      MSE_means[i] <- mse(test,point$forecast) }
    #return(colMeans(MSE_mat))
    return(MSE_mat)}
    

```

```{r}
#AAAemp_res <- cross_val_empirical('AAA' , 12 )
#MAAemp_res <- cross_val_empirical('MAA' , 12 )
MAMemp_res <- cross_val_empirical('MAM' , 12 )
MAM_means <- cross_val_mse('MAM' , 12)
#qqq <- cross_val_empirical('MAM' , 12 )


```

```{r}
#AAA_es <- es(train_set , model = AAA , h = 12 , damped = FALSE)
#MAA_es <- es(train_set , model = MAA , h = 12 , damped = FALSE)
MAM_es <- es(train_set , model = MAM , h = 12 , damped = FALSE)
flag <- TRUE
while (flag == TRUE){
  flag <- FALSE
  simmulationsMAM <- (matrix(0, 12 ,10000))
  for (i in 1:10000){
    simmulationsMAM[,i] <- simulate(MAM , 12 )
  }
  sim <- full_mean_kde(simmulationsMAM , 0.95)
  for (i in 1:2){
    if (sum(is.na(sim[[i]]))!=0){
      flag <- TRUE

    } }
}

flag <- TRUE
while (flag == TRUE){
  flag <- FALSE
  bootMAM <- (matrix(0, 12 ,10000))
  for (i in 1:10000){
    bootMAM[,i] <- simulate(MAM , 12 , bootstrap = TRUE )
  }
  boot <- full_mean_kde(bootMAM , 0.95)
  for (i in 1:2){
    if (sum(is.na(boot[[i]]))!=0){
      flag <- TRUE

    } }
}
```




**Results on the optimal MAM model: **

```{r}

no_mean_df <- MAMemp_res
no_mean_df['Algebric'] <- alg_res$MAM_cv[1:12]
par(bg = 'aliceblue')

plot(no_mean_df$h , no_mean_df$`Direct Empirical`, type='o', col='red' , ylim=c(40,260) ,pch = 1,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=2)
lines(no_mean_df$h , no_mean_df$`KDE Empirical` , col = 'blue ' ,lwd=1,type='o' ,pch=2)
#lines(no_mean_df$h , boot_res$`Mean KDE` , col = 'yellow ',lwd=1 ,type='o' ,pch=2)
#lines(no_mean_df$h , sim_res$`Mean KDE` , col = 'purple ' ,lwd=1 ,type='o',pch=3)
lines(no_mean_df$h , no_mean_df$Algebric , col = 'green',lwd=1 ,type='o',pch=3)
title(main = "Interval Scores Comparisson for MAM" , sub='Figure 4.1',cex.sub=0.9 , font.main=3 , col.sub='red3')

legend(0.7, 220, legend = c("ErrorsDirect" , "ErrorsKDE" ,"Algebric forecast")
         ,col=c("red", "blue" , "green"),lty=1, cex=0.8 , pch=c(0,1,2,4)) 


print(MAMemp_res)

```

Interestingly, the two empirical methods produced excelent results. Empirical KDE outperformed the algebric nearly in all horizons, while it performed better against the Empirical Direct, except for the 3 last horizons.

**The errors Distribution of the MAM model, without fitting the last year.**

Green stands for the Direct Empirical quantiles while Blue is for the KDE method 



```{r}
density_quant <- function(model , a ){
  a <- (1-a)/2 + c(0,1)*a 
  errors <- model$errors
  er <- errors[-(1:12),] 
  low <- rep(0,12)
  up <- rep(0,12)
  for (h in 1:12){
    density <- density.default(er[,h],bw='nrd0',kernel = "epanechnikov")
    cdensity <- cumsum(density$y)
    kcde <- cdensity/max(cdensity)
    
    x <- rep(0,2)
    for (i in 1:2){
      idx <- order(abs(kcde-a[i]))[1:2] 
      x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y 
      if (is.na(x[i])){
        idx <- order(abs(kcde-a[i]))[2:3] 
        x[i] <- approx(kcde[idx],density$x[idx],xout=a[i])$y         
      }
    }
    low[h] <- x[1]
    up[h] <- x[2]
  }
  return(list(low[1:12],up[1:12]))  
}

```


```{r, fig.height=4, fig.width=8}
par(mfrow=c(1,3),bg = 'aliceblue')
errors <- MAM_es$errors
er <- errors[-(1:12),]


up <- density_quant(MAM_es , 0.95)[[2]]
low <- density_quant(MAM_es , 0.95)[[1]]

for (i in 10:12){

  hist(er[,i] ,breaks=25, col='red' ,xlab ='Observation' ,
      main=paste("h = ",i) ,freq = FALSE)
  abline(v=quantile(er[,i],c(0.05,0.95))[1], col="green", lwd=2, lty=2 )
  abline(v=quantile(er[,i],c(0.05,0.95))[2], col="green", lwd=2, lty=2)
  abline(v=up[i], col="blue", lwd=2, lty=2 )
  abline(v=low[i], col="blue", lwd=2, lty=2)
  dens <- density(er[,i], bw = 'nrd0' , kernel ="epanechnikov")
  lines(dens,col = 'black' ,lwd=3 )
  

}
mtext("Figure 4.2", side = 1, line = -0.9, outer = TRUE , col = 'red3')
```

Interestigly, the empirical method produced narrower intervals, than estimating the distribution with a kernel function,as it threw away the observations on the tails of the distribution(values outside to 5% and 95% observation).

On the other hand,as KDE tries to smoothly include all the observations,due to some assymetries on the tails , it failed to do so and it included some extra values,resulting on wider intervals. 

As literature suggests, and from my point of view, a bigger sample size would result in a better fitted density function,as it would more accuretely fit on distrubution with more observations and hence, better results!




**Comparing the PIs of the different methods on MAM**

```{r, fig.height=8, fig.width=10}
par(mfrow=c(2,2),bg = 'aliceblue')
asd <- forecast(MAM_es , 12 )

#Error KDE
up<-mu_kde_quant(MAM_es , 0.95)[[2]]
low <-mu_kde_quant(MAM_es , 0.95)[[1]]

plot(c(1:12),test_set , type='l' , lwd = 2 , col='red' , xlab='Horizon' , ylab ='Value' ,ylim=c(330,700) , main = "Empirical KDE")
lines(c(1:12),up,lwd=2)
lines(c(1:12),low,lwd=2)
lines(c(1:12) , asd$mean , lwd = 2 ,col = 'blue')
#legend(1,550,legend = c("True","Predicted") , col = c("red" , "blue "),lty=1 )
polygon(c(c(1:12), rev(c(1:12))),c(up, rev(low)), 
        density = c(20, 30),
     col = 'yellow3', border = NA)

#Boot KDE

up<-boot[[2]]
low <-boot[[1]]

plot(c(1:12),test_set , type='l' , lwd = 2 , col='red' , xlab='Horizon' , ylab ='Value' ,ylim=c(330,700) , main = "Mean-KDE on Bootsrap")
lines(c(1:12),up,lwd=2)
lines(c(1:12),low,lwd=2)
lines(c(1:12) , asd$mean , lwd = 2 ,col = 'blue')
#legend(1,550,legend = c("True","Predicted") , col = c("red" , "blue "),lty=1 )
polygon(c(c(1:12), rev(c(1:12))),c(up, rev(low)), 
        density = c(20, 30),
     col = 'yellow3', border = NA)

#Error KDE
up<-sim[[2]]
low <-sim[[1]]

plot(c(1:12),test_set , type='l' , lwd = 2 , col='red' , xlab='Horizon' , ylab ='Value' ,ylim=c(330,700) , main = "Mean - KDE on Sim")
lines(c(1:12),up,lwd=2)
lines(c(1:12),low,lwd=2)
lines(c(1:12) , asd$mean , lwd = 2 ,col = 'blue')
#legend(1,550,legend = c("True","Predicted") , col = c("red" , "blue "),lty=1 )
polygon(c(c(1:12), rev(c(1:12))),c(up, rev(low)), 
        density = c(20, 30),
     col = 'yellow3', border = NA)

#Boot KDE

up<-algebricMAM$upper[,2]
low <-algebricMAM$lower[,2]

plot(c(1:12),test_set , type='l' , lwd = 2 , col='red' , xlab='Horizon' , ylab ='Value' ,ylim=c(330,700) , main = "Algebric ")
lines(c(1:12),up,lwd=2)
lines(c(1:12),low,lwd=2)
lines(c(1:12) , asd$mean , lwd = 2 ,col = 'blue')
#legend(1,550,legend = c("True","Predicted") , col = c("red" , "blue "),lty=1 )
polygon(c(c(1:12), rev(c(1:12))),c(up, rev(low)), 
        density = c(20, 30),
     col = 'yellow3', border = NA)

mtext("Figure 4.3", side = 1, line = -1.5, outer = TRUE , col = 'red3')
```

A more narrow PI produced by Empirical KDE, in comparssion with the other methods. This is because the lower interval has been "moved" closer to the mean forecast(blue line).

**Comparing the CrossValidated Interval Scores :**

```{r}
validated <- data.frame(h)

validated['Algebric'] <- alg_res$MAM_cv
validated['Sim_Empirical'] <- MAMsim_res$`Direct EMP`
validated['Sim KDE'] <- MAMsim_res$`Mean KDE`
validated['Boot Empir'] <- MAMboot_res$`Direct EMP`
validated['Boot KDE'] <- MAMboot_res$`Mean KDE`
validated['Error Empirical'] <- MAMemp_res$`Direct Empirical`
validated['Error KDE'] <- MAMemp_res$`KDE Empirical`

print(validated)
```

```{r, fig.height=5, fig.width=8}
#Getting the method that produced the best MIS on the simmulation method!

par(bg = 'aliceblue')
plot(c(1:12) , validated$`Algebric`, type='l', col='red' , ylim=c(50,270) ,pch = 1,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=1.5)
lines(c(1:12) , validated$`Sim_Empirical` , col = 'blue ' ,lwd=0.5 ,type='o' ,pch=2)
lines(c(1:12) , validated$`Sim KDE` , col = 'yellow ',lwd=1 ,type='o' ,pch=3)
lines(c(1:12) , validated$`Boot Empir` , col = 'purple ' ,lwd=1 ,type='o',pch=4)
lines(c(1:12), validated$`Boot KDE`, col = 'green',lwd=1 ,type='o',pch=5)
lines(c(1:12), validated$`Error Empirical`, col = 'black',lwd=1 ,type='o',pch=6)
lines(c(1:12), validated$`Error KDE`, col = 'brown',lwd=1 ,type='o',pch=7)

title(main = "Methods Comparisson for MAM" , sub='Figure 4.4',cex.sub=0.9 , font.main=3 , col.sub='red3')

legend("topleft", legend = c("Algebric" ,"Sim Empirical","Sim KDE" , "Boot Empirical", "Boot KDE",
                            "Error Empirical" , "Error KDE")
         ,col=c("red", "blue" , "yellow" , "purple" , "green" , "black" , "brown"),
       lty=1, cex=0.8 , pch=c(1,2,3,4,5,6,7)) 
print(validated)

```

By considering figures 4.3 and 4.4 allong with the table above empirical methods, which explore past errors in order to forecast future uncertainty, are the best methods when an accurate prediction interval is required and hence, they are highly recommended.

The Errors-KDE method produced the better overall results, with error-empirical performing better for h > 9. A possible explanation for such behavior is given on figure 4.2. 

A limitation of the empirical method is that it concentrates on the tails of the distribution and hence, not taking into account the complete shape of the probability distribution. As a result and after considering the results on figure 4.4 and the literature's suggestions,that KDE performs better than Empirical bigger sample sizes, KDE method is prefered.

However, as the empirical method produced good results for h > 9, both methods should be consider and depending on the sample size and the forecast horizon, one or the other are highly recommended.

What is more, both the bootstrap and the simmulation methods produced equal good-enough results, with the simmulation methods performing a bit better. Interestingly, there is not a vast difference between their scores and the scores of the algebric method. 

Such results, question the use of computational resources for forecasting intervals through simmulation and bootsrap instead of using empirical methods, which on top of the better results are based on a much "lighter" assumption than the other two. Despite this hypothesis, bootstrap method overlaps the normality assumption and hence is prefered from the other two!

To sum things up, when a fast,naive and not so accurate forecast of a prediction interval is needed,or when time and complex calculations(show relative computation times for proof) are not accesible, the theoretical algebric method is recommended. However, it should be used with caution, as many of the assumptions needed, might not be feasible!

If its implementation is not feasible, an alternative is the boostrap method with either Mean-KDE or Mean-Empirical depending the sample size and the forecast horizon. 

When a more accurate forecast of prediction interval is needed, assuming that future errors would follow a simmilar pattern like past errors(a feasible assumption to be made) and exploring the behaviour of past errors to add them to future point forecast is the best method. Depending on the sample size, the forecast horizon and the particular series, a choice of either KDE or direct quant method could be made.



**Comparing the scores of the optimal MAM model vs two other non-optimal ones**

All methods described above will be used on two non Optimal ETS models(AAA and MAA) to validate the importance of using an optimal model when it comes to producing accurate Intervals.

GMRAE will be used

```{r}
#Adjusting the functions:
cross_val_algebric_adjusted <- function(model , h ){
  #Initializing
  train <- window(AirPassengers , end = 1958.99)
  test <- window(AirPassengers , start = 1959)
  n <- length(test) - h + 1
  
  lower <- rep(NA , 12)
  upper <- rep(NA , 12)
  #Every row is the IS of a 12-steps-ahead forecast
  IS_mat <- matrix(NA , nrow = n , ncol = h)
  fit <- ets(train , model = model)
  for(i in 1:n){
  #Adjusting the training set to include a new observation each time!
    #New training set:
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    #New test set:
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    #Refitting:
    refit <- ets(x , model = model)
    #New forecast:
    new_forecast <- forecast(refit , h = h)
    #New intervals:
    lower <- new_forecast$lower[,2]
    upper <- new_forecast$upper[,2]
    IS_mat[i , ] <- ISs(test , upper , lower , 0.05)
    #Every row is a forecast for h = 12 
    #In total I have 13 forecasts to evaluate
  }
  #The mean of every collumn is the mean IS for every every horizon h
  
return(IS_mat)
}

GMIS <- function(score,benchmark_score){
  EnvStats::geoMean(abs(score/benchmark_score))
}

GMRAE_algebric <- function(model , h){
  benchmark <- cross_val_algebric_adjusted('MAM' , 12)
  results <- cross_val_algebric_adjusted(model , 12)
  errors <- rep(NA , 12 )
  for (i in 1:12){
    errors[i] <- GMIS(results[,i],benchmark[,i])
  }
  return(errors)
}


```


```{r}
cross_val_simmulation_adjusted <- function(model , h , to_bootstrap){
  #A dataframe for all scores
  to_return <- data.frame(h = c(1:12))
  #Initializing
  train <- window(AirPassengers , end = 1958.99)
  test <- window(AirPassengers , start = 1959)
  n <- length(test) - h + 1
  lower <- rep(NA , 12)
  upper <- rep(NA , 12)
  #A matrix for each on of the methods
  DirectEMP_mat <- matrix(NA , nrow = n , ncol = h)
  MeanKDE_mat <- matrix(NA , nrow = n , ncol = h)
  fit <- ets(train , model = model)
  for(i in 1:n){
    #Initializing cross validation 
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    refit <- ets(x , model = model)
    #Simmulating for the fitted model
    sim_results <- single_row_results(refit , test , h , to_bootstrap)
    DirectEMP_mat[i,] <- sim_results[[1]]
    MeanKDE_mat[i,] <- sim_results[[4]]
    
  }

  return(list(DirectEMP_mat,MeanKDE_mat)) }

GMRAE_sim <- function(model , h , to_bootsrap = FALSE){
  benchmark_emp <- cross_val_simmulation_adjusted('MAM' , 12,to_bootsrap)[[1]]
  benchmark_kde <- cross_val_simmulation_adjusted('MAM' , 12,to_bootsrap)[[2]]
  results_emp <- cross_val_simmulation_adjusted(model , 12,to_bootsrap)[[1]]
  results_kde <- cross_val_simmulation_adjusted(model , 12,to_bootsrap)[[2]]
  
  errors_emp <- rep(NA , 12 )
  errors_kde <- rep(NA , 12 )
  for (i in 1:12){
    errors_emp[i] <- GMIS(results_emp[,i],benchmark_emp[,i])
    errors_kde[i] <- GMIS(results_kde[,i],benchmark_kde[,i])
  }
  return(list(errors_emp,errors_kde))
}


```

```{r}
cross_val_empirical_adjusted <- function(model , h ,multi_error=TRUE){
    to_return <- data.frame(h = c(1:12))
    #Initializing
    train <- window(AirPassengers , end = 1958.99)
    test <- window(AirPassengers , start = 1959)
    n <- length(test) - h + 1
    lower <- rep(NA , 12)
    upper <- rep(NA , 12)
    #A matrix for each on of the methods
    EMP_mat <- matrix(NA , nrow = n , ncol = h)
    KDE_mat <- matrix(NA , nrow = n , ncol = h)
    fit <- ets(train , model = model)
    #Converting to es
    model_es <- es(train , model = fit , h = 12)
  for(i in 1:13){
    #Initializing cross validation 
    x <- window(AirPassengers , end = 1958.99 + (i-1)/12)
    test <- as.double(window(AirPassengers , start = 1959 + (i-1)/12 , end = 1959.99 + (i-1)/12))
    refit <- ets(x , model = model)
    refit_es <- es(x , model = refit , h = 12)
    #Getting results for each origin
    if (multi_error==TRUE){
      upper_emp <- mu_direct_quant(refit_es , 0.95)[[2]]
      lower_emp <- mu_direct_quant(refit_es , 0.95)[[1]]
      EMP_mat[i,] <- ISs(test, upper_emp , lower_emp , 0.05)
      
      upper_kde <- mu_kde_quant(refit_es , 0.95)[[2]]
      lower_kde <- mu_kde_quant(refit_es , 0.95)[[1]]
      KDE_mat[i,] <- ISs(test, upper_kde , lower_kde , 0.05)
    }
    else {
      upper_emp <- direct_quant(refit_es , 0.95)[[2]]
      lower_emp <- direct_quant(refit_es , 0.95)[[1]]
      EMP_mat[i,] <- ISs(test, upper_emp , lower_emp , 0.05)
      
      upper_kde <- kde_quant(refit_es , 0.95)[[2]]
      lower_kde <- kde_quant(refit_es , 0.95)[[1]]
      KDE_mat[i,] <- ISs(test, upper_kde , lower_kde , 0.05)      
    }
  }
  
  return(list(EMP_mat,KDE_mat))
}
GMRAE_emp <- function(model , h , multi_error=TRUE){
  benchmark_emp <- cross_val_empirical_adjusted('MAM' , 12)[[1]]
  benchmark_kde <- cross_val_empirical_adjusted('MAM' , 12)[[2]]

  results_emp <- cross_val_empirical_adjusted(model , 12, multi_error)[[1]]
  results_kde <- cross_val_empirical_adjusted(model , 12, multi_error)[[2]]
  
  errors_emp <- rep(NA , 12 )
  errors_kde <- rep(NA , 12 )
  for (i in 1:12){
    errors_emp[i] <- GMIS(results_emp[,i],benchmark_emp[,i])
    errors_kde[i] <- GMIS(results_kde[,i],benchmark_kde[,i])
  }
  return(list(errors_emp,errors_kde))
}


```








```{r}
AAA_geom <- data.frame(h=c(1:12))
MAA_geom <- data.frame(h = c(1:12))



AAA_geom['Algebric'] <- GMRAE_algebric('AAA',12)
MAA_geom['Algebric'] <- GMRAE_algebric('MAA',12)

AAA_sim <- GMRAE_sim('AAA',12)
AAA_geom['Sim Emp'] <- AAA_sim[[1]]
AAA_geom['Sim KDE'] <- AAA_sim[[2]]

MAA_sim <- GMRAE_sim('MAA',12)
MAA_geom['Sim Emp'] <- MAA_sim[[1]]
MAA_geom['Sim KDE'] <- MAA_sim[[2]]

AAA_bot <- GMRAE_sim('AAA',12 , TRUE)
AAA_geom['Boot Emp'] <- AAA_bot[[1]]
AAA_geom['Boot KDE'] <- AAA_bot[[2]]

MAA_bot <- GMRAE_sim('MAA',12, TRUE)
MAA_geom['Boot Emp'] <- MAA_bot[[1]]
MAA_geom['Boot KDE'] <- MAA_bot[[2]]

MAA_emp <- GMRAE_emp('MAA',12)
MAA_geom['Error Emp'] <- MAA_emp[[1]]
MAA_geom['Error KDE'] <- MAA_emp[[2]]

AAA_emp <- GMRAE_emp('AAA',12 , FALSE)
AAA_geom['Error Emp'] <- AAA_emp[[1]]
AAA_geom['Error KDE'] <- AAA_emp[[2]]




```

**For AAA: **

```{r, fig.height=5, fig.width=8}
#Getting the method that produced the best MIS on the simmulation method!
AAA_geom[13,] <- colMeans(AAA_geom)
no_mean_df <- AAA_geom

par(bg = 'aliceblue')
plot(c(1:12) , no_mean_df[1:12,'Algebric'], type='l', col='red' , ylim=c(0.9,4) ,pch = 1,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=1.5)
lines(c(1:12) , no_mean_df[1:12,'Sim Emp'] , col = 'blue ' ,lwd=0.5 ,type='o' ,pch=2)
lines(c(1:12) , no_mean_df[1:12,'Sim KDE'] , col = 'yellow ',lwd=1 ,type='o' ,pch=3)
lines(c(1:12) , no_mean_df[1:12,'Boot Emp'] , col = 'purple ' ,lwd=1 ,type='o',pch=4)
lines(c(1:12), no_mean_df[1:12,'Boot KDE'], col = 'green',lwd=1 ,type='o',pch=5)
lines(c(1:12), no_mean_df[1:12,'Error Emp'], col = 'black',lwd=1 ,type='o',pch=6)
lines(c(1:12), no_mean_df[1:12,'Error KDE'], col = 'brown',lwd=1 ,type='o',pch=7)
title(main = "GMRAE between Optimal MAM and AAA" ,
      sub = paste("Mean Difference:" , round(mean(as.double(AAA_geom[13,2:8]))),2),cex.sub=0.9 , font.main=3 , col.sub='red3')

legend(0.6, 4.1 , legend = c(paste("Algebric mean:",round(AAA_geom[13,'Algebric'],2)) ,
                            paste("Sim EMP",round(AAA_geom[13,"Sim Emp"],2)),
                           paste("Sim KDE",round(AAA_geom[13,"Sim KDE"],2)), 
                           paste("Boot Emp",round(AAA_geom[13,"Boot Emp"],2)),
                           paste("Boot KDE",round(AAA_geom[13,"Boot KDE"],2)),
                            paste("Error Emp",round(AAA_geom[13,"Error Emp"],2)),
                            paste("Error KDE",round(AAA_geom[13,"Error KDE"],2)) )
                              
         ,col=c("red", "blue" , "yellow" ,"purple","green"),lty=1, cex=0.8 , pch=c(0,1,2,3,4,5)) 

```


why non optimal methods perform worse? mean forecast + quantile. show nubmers


**For MAM:**

```{r, fig.height=5, fig.width=8}
MAA_geom[13,] <- colMeans(MAA_geom)
no_mean_df <- MAA_geom

par(bg = 'aliceblue')
plot(c(1:12) , no_mean_df[1:12,'Algebric'], type='l', col='red' , ylim=c(0.9,4) ,pch = 1,
      xlab="Horizon h" , ylab = "Interval Scores",lwd=1.5)
lines(c(1:12) , no_mean_df[1:12,'Sim Emp'] , col = 'blue ' ,lwd=0.5 ,type='o' ,pch=2)
lines(c(1:12) , no_mean_df[1:12,'Sim KDE'] , col = 'yellow ',lwd=1 ,type='o' ,pch=3)
lines(c(1:12) , no_mean_df[1:12,'Boot Emp'] , col = 'purple ' ,lwd=1 ,type='o',pch=4)
lines(c(1:12), no_mean_df[1:12,'Boot KDE'], col = 'green',lwd=1 ,type='o',pch=5)
lines(c(1:12), no_mean_df[1:12,'Error Emp'], col = 'black',lwd=1 ,type='o',pch=6)
lines(c(1:12), no_mean_df[1:12,'Error KDE'], col = 'brown',lwd=1 ,type='o',pch=7)
title(main = "GMRAE between Optimal MAM and MAA" ,
      sub = paste("Mean Difference:" , round(mean(as.double(MAA_geom[13,2:8]))),2),cex.sub=0.9 , font.main=3 , col.sub='red3')

legend(0.6, 4.1 , legend = c(paste("Algebric mean:",round(MAA_geom[13,'Algebric'],2)) ,
                            paste("Sim EMP",round(MAA_geom[13,"Sim Emp"],2)),
                           paste("Sim KDE",round(MAA_geom[13,"Sim KDE"],2)), 
                           paste("Boot Emp",round(MAA_geom[13,"Boot Emp"],2)),
                           paste("Boot KDE",round(MAA_geom[13,"Boot KDE"],2)),
                            paste("Error Emp",round(MAA_geom[13,"Error Emp"],2)),
                            paste("Error KDE",round(MAA_geom[13,"Error KDE"],2)) )
                              
         ,col=c("red", "blue" , "yellow" ,"purple","green"),lty=1, cex=0.8 , pch=c(0,1,2,3,4,5)) 

```

A mean difference of 2.2 is identified for both models across all models and every methods has an error of over 1.5.  Moreover, for each horizon, no model has outperformed MAM

This proves that not picking an optimal model adds extra bias on the intervals forecast, which highly depends on how good does a model fit the given time series. 

**In order to get accurate prediction interval forecasts, using the optimal model in terms of Information Criteria is neccessary**

If picking the optimal model is not feasible, then the forecasted intervals would include some extra bias which is a result of the uncertainty in selecting the optimal model for forecasting.



